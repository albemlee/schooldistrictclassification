{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Working Directory\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The US Primary and Secondary School System (public and charter elementary, middle, and high schools) is very large. However, the scale of the education system is often lost in our discussions. Figure 1 below illustrates the size of the US Primary and Secondary School System by comparing it with [US Defense](https://en.wikipedia.org/wiki/Military_budget_of_the_United_States).\n",
    "\n",
    "![Figure 1: Overall government spending on defense and education (primary/secondary) are comparable.](images/spendingdefenseeducation.jpg)\n",
    "\n",
    "US primary and secondary education's total funding is comparable to the US military budget, which is by far, the largest in the world. However, unlike the military, which is federally funded, education is mostly funded by local and state governments.\n",
    "\n",
    "Because of education's scale, seemingly minor phenomena can have a major impact. As shown in Figure 2 below, the vast majority of 2010 school districts were still operational in 2015. However, the mere 4 percent of school districts that closed accounted for over $4 billions in government spending, and more importantly, about 380,000 students faced disruptions which could [harm communities](https://www.nytimes.com/2018/06/13/us/arena-wisconsin-schools-empty.html).\n",
    "\n",
    "![Figure 2: More than 95 percent of school districts in 2010 remained operational in 2015.](images/2010pie.png)\n",
    "\n",
    "By examining these \"at-risk\" school districts, education agencies and non-profit organizations can determine actions which may mitigate the effects of disruptions. To conduct studies on at-risk school districts, researchers must first identify them. Machine learning can be used to predict the likelihood of school districts closing within five years, and I created a binary classification model.\n",
    "\n",
    "To complete the task, I completed the following steps.\n",
    "1. Load the Data: Find and acquire data for analysis.\n",
    "2. Wrangle the Data: Convert the data into a format suitable for further analysis.\n",
    "3. Explore the Data: Identify patterns, develop an understanding of underlying distributions, and select variables most relevant for completing the task.\n",
    "4. Build and Select Machine Learning Model: Establish evaluation metric, build multiple models, and select the best performing model based on the metric.\n",
    "5. Provide Recommendations: Provide guidelines for using insights generated by the model, and provide recommendations for further work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "To complete this supervised learning task, I used the National Center of Education Statistic's [Common Core of Data](https://nces.ed.gov/ccd/). The Common Core of Data contains all information on primary and secondary school districts used by the Department of Education. It covers years between 1990 and 2015, and it contains hundreds of variables such as student demographics, revenue sources, and spending categories.\n",
    "\n",
    "The data is stored in separate tab-delimited files on CCD's website. I downloaded the files from the website, and then stored the files within zipped archives. Table 1 below lists the files I used for this project.\n",
    "\n",
    "![Table 1: The data was stored as separate tab-delimited text files on CCD's website.](images/files.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I completed data wrangling tasks with Python and Pandas. Before proceeding, I loaded the files into [Pandas DataFrames](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile # I used the zipfile library to access compressed archives\n",
    "\n",
    "# Compressed archives were stored in the \"data\" folder within the project directory\n",
    "finance_archive = zipfile.ZipFile('data/finance.zip', 'r')\n",
    "universe_archive = zipfile.ZipFile('data/universe.zip', 'r')\n",
    "\n",
    "# Read finance data\n",
    "finance_2010 = pd.read_csv(\n",
    "    finance_archive.open('finance_2010.txt'),\n",
    "    dtype='str',\n",
    "    delimiter='\\t',\n",
    "    encoding='windows-1252')\n",
    "finance_2011 = pd.read_csv(\n",
    "    finance_archive.open('finance_2011.txt'),\n",
    "    dtype='str',\n",
    "    delimiter='\\t',\n",
    "    encoding='windows-1252')\n",
    "\n",
    "# Read universe data\n",
    "universe_2010 = pd.read_csv(\n",
    "    universe_archive.open('universe_2010.txt'),\n",
    "    dtype='str',\n",
    "    delimiter='\\t',\n",
    "    encoding='windows-1252')\n",
    "universe_2011 = pd.read_csv(\n",
    "    universe_archive.open('universe_2011.txt'),\n",
    "    dtype='str',\n",
    "    delimiter='\\t',\n",
    "    encoding='windows-1252')\n",
    "\n",
    "# Read school universe data\n",
    "school_universe_2010 = pd.read_csv(\n",
    "    universe_archive.open('school_universe_2010.txt'),\n",
    "    dtype='str',\n",
    "    delimiter='\\t',\n",
    "    encoding='windows-1252')\n",
    "\n",
    "# Read directory data\n",
    "universe_2015_directory = pd.read_csv(\n",
    "    universe_archive.open('universe_2015_directory.txt'),\n",
    "    dtype='str',\n",
    "    delimiter='\\t',\n",
    "    encoding='windows-1252')\n",
    "universe_2016_directory = pd.read_csv(\n",
    "    universe_archive.open('universe_2016_directory.txt'),\n",
    "    dtype='str',\n",
    "    delimiter=',',\n",
    "    encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the data was correctly loaded into DataFrames, I printed the shape of each DataFrame I created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finance_2010:             (18247, 256)\n",
      "finance_2011:             (18297, 256)\n",
      "universe_2010:            (18439, 58)\n",
      "universe_2011:            (18478, 319)\n",
      "school_universe_2010:     (103959, 289)\n",
      "universe_2015_directory:  (18834, 56)\n",
      "universe_2016_directory:  (18893, 57)\n"
     ]
    }
   ],
   "source": [
    "# print shape of each DataFrame\n",
    "print('finance_2010:            ', finance_2010.shape)\n",
    "print('finance_2011:            ', finance_2011.shape)\n",
    "print('universe_2010:           ', universe_2010.shape)\n",
    "print('universe_2011:           ', universe_2011.shape)\n",
    "print('school_universe_2010:    ', school_universe_2010.shape)\n",
    "print('universe_2015_directory: ', universe_2015_directory.shape)\n",
    "print('universe_2016_directory: ', universe_2016_directory.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 18,000 school districts, and there are hundreds of variables available for each school district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling the Data\n",
    "\n",
    "Right now, the data is stored in multiple DataFrames. I wrangled the data from each school year into a [tidy](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) DataFrame to simplify analysis. Figure 3 below summarizes the process I used to transform the data into tidy data.\n",
    "\n",
    "![Figure 3: Tidying the data involved aggregating, merging, labeling, and encoding.](images/preprocessing_steps.png)\n",
    "\n",
    "The preprocessing steps are very similar between the 2010 and 2011 datasets. The main difference is that 2010 demographic information is only available at the school level, and it needs to be aggregated by the school district level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating School Universe Data (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record within the school universe files represents a single school. For my tidy dataset, each record will represent a school district, so I aggregated the school universe data by school districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate school_universe_2010\n",
    "# Specify types \n",
    "df_types = {\n",
    "    'NCESSCH': 'str', 'FIPST': 'str', 'LEAID': 'str', 'SCHNO': 'str', 'STID': 'str', 'SEASCH': 'str', \n",
    "    'LEANM': 'str', 'SCHNAM': 'str', 'PHONE': 'str', 'MSTREE': 'str', 'MCITY': 'str', 'MSTATE': 'str', \n",
    "    'MZIP': 'str', 'MZIP4': 'str', 'LSTREE': 'str', 'LCITY': 'str', 'LSTATE': 'str', 'LZIP': 'str', \n",
    "    'LZIP4': 'str', 'TYPE': 'str', 'STATUS': 'str', 'ULOCAL': 'str', 'LATCOD': 'str', 'LONCOD': 'str', \n",
    "    'CONUM': 'str', 'CONAME': 'str', 'CDCODE': 'str', 'FTE': 'float', 'GSLO': 'str', 'GSHI': 'str', \n",
    "    'LEVEL': 'str', 'TITLEI': 'str', 'STITLI': 'str', 'MAGNET': 'str', 'CHARTR': 'str', 'SHARED': 'str', \n",
    "    'BIES': 'str', 'FRELCH': 'float', 'REDLCH': 'float', 'TOTFRL': 'float', 'RACECAT': 'str', 'PK': 'float', \n",
    "    'AMPKM': 'float', 'AMPKF': 'float', 'ASPKM': 'float', 'ASPKF': 'float', 'HIPKM': 'float', 'HIPKF': 'float', \n",
    "    'BLPKM': 'float', 'BLPKF': 'float', 'WHPKM': 'float', 'WHPKF': 'float', 'HPPKM': 'float', 'HPPKF': 'float', \n",
    "    'TRPKM': 'float', 'TRPKF': 'float', 'KG': 'float', 'AMKGM': 'float', 'AMKGF': 'float', 'ASKGM': 'float', \n",
    "    'ASKGF': 'float', 'HIKGM': 'float', 'HIKGF': 'float', 'BLKGM': 'float', 'BLKGF': 'float', 'WHKGM': 'float', \n",
    "    'WHKGF': 'float', 'HPKGM': 'float', 'HPKGF': 'float', 'TRKGM': 'float', 'TRKGF': 'float', 'G01': 'float', \n",
    "    'AM01M': 'float', 'AM01F': 'float', 'AS01M': 'float', 'AS01F': 'float', 'HI01M': 'float', 'HI01F': 'float',\n",
    "    'BL01M': 'float', 'BL01F': 'float', 'WH01M': 'float', 'WH01F': 'float', 'HP01M': 'float', 'HP01F': 'float', \n",
    "    'TR01M': 'float', 'TR01F': 'float', 'G02': 'float', 'AM02M': 'float', 'AM02F': 'float', 'AS02M': 'float', \n",
    "    'AS02F': 'float', 'HI02M': 'float', 'HI02F': 'float', 'BL02M': 'float', 'BL02F': 'float', 'WH02M': 'float', \n",
    "    'WH02F': 'float', 'HP02M': 'float', 'HP02F': 'float', 'TR02M': 'float', 'TR02F': 'float', 'G03': 'float', \n",
    "    'AM03M': 'float', 'AM03F': 'float', 'AS03M': 'float', 'AS03F': 'float', 'HI03M': 'float', 'HI03F': 'float', \n",
    "    'BL03M': 'float', 'BL03F': 'float', 'WH03M': 'float', 'WH03F': 'float', 'HP03M': 'float', 'HP03F': 'float', \n",
    "    'TR03M': 'float', 'TR03F': 'float', 'G04': 'float', 'AM04M': 'float', 'AM04F': 'float', 'AS04M': 'float', \n",
    "    'AS04F': 'float', 'HI04M': 'float', 'HI04F': 'float', 'BL04M': 'float', 'BL04F': 'float', 'WH04M': 'float', \n",
    "    'WH04F': 'float', 'HP04M': 'float', 'HP04F': 'float', 'TR04M': 'float', 'TR04F': 'float', 'G05': 'float', \n",
    "    'AM05M': 'float', 'AM05F': 'float', 'AS05M': 'float', 'AS05F': 'float', 'HI05M': 'float', 'HI05F': 'float', \n",
    "    'BL05M': 'float', 'BL05F': 'float', 'WH05M': 'float', 'WH05F': 'float', 'HP05M': 'float', 'HP05F': 'float', \n",
    "    'TR05M': 'float', 'TR05F': 'float', 'G06': 'float', 'AM06M': 'float', 'AM06F': 'float', 'AS06M': 'float', \n",
    "    'AS06F': 'float', 'HI06M': 'float', 'HI06F': 'float', 'BL06M': 'float', 'BL06F': 'float', 'WH06M': 'float', \n",
    "    'WH06F': 'float', 'HP06M': 'float', 'HP06F': 'float', 'TR06M': 'float', 'TR06F': 'float', 'G07': 'float', \n",
    "    'AM07M': 'float', 'AM07F': 'float',  'AS07M': 'float', 'AS07F': 'float', 'HI07M': 'float', 'HI07F': 'float',  \n",
    "    'BL07M': 'float', 'BL07F': 'float', 'WH07M': 'float', 'WH07F': 'float', 'HP07M': 'float', 'HP07F': 'float', \n",
    "    'TR07M': 'float', 'TR07F': 'float', 'G08': 'float', 'AM08M': 'float', 'AM08F': 'float', 'AS08M': 'float', \n",
    "    'AS08F': 'float', 'HI08M': 'float', 'HI08F': 'float', 'BL08M': 'float', 'BL08F': 'float', 'WH08M': 'float', \n",
    "    'WH08F': 'float', 'HP08M': 'float', 'HP08F': 'float', 'TR08M': 'float', 'TR08F': 'float', 'G09': 'float', \n",
    "    'AM09M': 'float', 'AM09F': 'float', 'AS09M': 'float', 'AS09F': 'float', 'HI09M': 'float', 'HI09F': 'float', \n",
    "    'BL09M': 'float', 'BL09F': 'float', 'WH09M': 'float', 'WH09F': 'float', 'HP09M': 'float', 'HP09F': 'float', \n",
    "    'TR09M': 'float', 'TR09F': 'float', 'G10': 'float', 'AM10M': 'float', 'AM10F': 'float', 'AS10M': 'float', \n",
    "    'AS10F': 'float', 'HI10M': 'float', 'HI10F': 'float', 'BL10M': 'float', 'BL10F': 'float', 'WH10M': 'float', \n",
    "    'WH10F': 'float', 'HP10M': 'float', 'HP10F': 'float', 'TR10M': 'float', 'TR10F': 'float', 'G11': 'float', \n",
    "    'AM11M': 'float', 'AM11F': 'float', 'AS11M': 'float', 'AS11F': 'float', 'HI11M': 'float', 'HI11F': 'float', \n",
    "    'BL11M': 'float', 'BL11F': 'float', 'WH11M': 'float', 'WH11F': 'float', 'HP11M': 'float', 'HP11F': 'float', \n",
    "    'TR11M': 'float', 'TR11F': 'float', 'G12': 'float', 'AM12M': 'float', 'AM12F': 'float', 'AS12M': 'float', \n",
    "    'AS12F': 'float', 'HI12M': 'float', 'HI12F': 'float', 'BL12M': 'float', 'BL12F': 'float', 'WH12M': 'float', \n",
    "    'WH12F': 'float', 'HP12M': 'float', 'HP12F': 'float', 'TR12M': 'float', 'TR12F': 'float', 'UG': 'float', \n",
    "    'AMUGM': 'float', 'AMUGF': 'float', 'ASUGM': 'float', 'ASUGF': 'float', 'HIUGM': 'float', 'HIUGF': 'float', \n",
    "    'BLUGM': 'float', 'BLUGF': 'float', 'WHUGM': 'float', 'WHUGF': 'float', 'HPUGM': 'float', 'HPUGF': 'float', \n",
    "    'TRUGM': 'float', 'TRUGF': 'float', 'MEMBER': 'float', 'AM': 'float', 'AMALM': 'float', 'AMALF': 'float', \n",
    "    'ASIAN': 'float', 'ASALM': 'float', 'ASALF': 'float', 'HISP': 'float', 'HIALM': 'float', 'HIALF': 'float', \n",
    "    'BLACK': 'float', 'BLALM': 'float', 'BLALF': 'float', 'WHITE': 'float', 'WHALM': 'float', 'WHALF': 'float', \n",
    "    'PACIFIC': 'float', 'HPALM': 'float', 'HPALF': 'float', 'TR': 'float', 'TRALM': 'float', 'TRALF': 'float', \n",
    "    'TOTETH': 'float'\n",
    "}\n",
    "\n",
    "# Convert float columns to float\n",
    "float_columns = []\n",
    "school_universe_2010.columns = df_types.keys()\n",
    "for column, dtype in df_types.items():\n",
    "    if dtype == 'float':\n",
    "        float_columns.append(column)\n",
    "        school_universe_2010[column] = school_universe_2010[column].astype(float)\n",
    "        \n",
    "# Aggregate (sum) float columns by school district('LEAID')\n",
    "float_columns.append('LEAID')\n",
    "school_universe_2010 = school_universe_2010[float_columns].groupby('LEAID').sum(skipna=True, min_count=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the school universe data was correctly aggregated, I printed the shape of the updated DataFrames. Since each record should now represent a school district, there should be around 18,000 records in each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school_universe_2010: (17177, 253)\n"
     ]
    }
   ],
   "source": [
    "print('school_universe_2010:', school_universe_2010.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are fewer school districts in the school universe data than the universe data. Upon further inspection, I discovered this is because many school districts do not have any schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Districts with no schools 2010: 1456\n"
     ]
    }
   ],
   "source": [
    "print('Districts with no schools 2010:', len(universe_2010[universe_2010['SCH09'] == '0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Data\n",
    "\n",
    "Before merging the DataFrames, I first standardized the column names (i.e. \"GSLO09\" and \"GSLO10\" should both be \"GSLO\"). I also converted columns into the proper types to ensure proper aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonize columns\n",
    "\n",
    "# universe_2010\n",
    "# Specify types \n",
    "df_types = {\n",
    "    'LEAID': 'str', 'FIPST': 'str', 'STID': 'str', 'NAME': 'str', 'PHONE': 'str', 'MSTREE': 'str', 'MCITY': 'str',\n",
    "    'MSTATE': 'str', 'MZIP': 'str', 'MZIP4': 'str', 'LSTREE': 'str', 'LCITY': 'str', 'LSTATE': 'str', 'LZIP': 'str',\n",
    "    'LZIP4': 'str', 'TYPE': 'str', 'UNION': 'str', 'CONUM': 'str', 'CONAME': 'str', 'CSA': 'str', 'CBSA': 'str',\n",
    "    'METMIC': 'str', 'ULOCAL': 'str', 'CDCODE': 'str', 'LATCOD': 'float', 'LONCOD': 'float', 'BOUND': 'str',\n",
    "    'BIEA': 'str', 'GSLO': 'str', 'GSHI': 'str', 'AGCHRT': 'str', 'SCH': 'float', 'UG': 'float', 'PK12': 'float',\n",
    "    'MEMBER': 'float', 'SPECED': 'float', 'ELL': 'float', 'PKTCH': 'float', 'KGTCH': 'float', 'ELMTCH': 'float',\n",
    "    'SECTCH': 'float', 'UGTCH': 'float', 'TOTTCH': 'float', 'AIDES': 'float', 'CORSUP': 'float', 'ELMGUI': 'float',\n",
    "    'SECGUI': 'float', 'OTHGUI': 'float', 'TOTGUI': 'float', 'LIBSPE': 'float', 'LIBSUP': 'float', 'LEAADM': 'float',\n",
    "    'LEASUP': 'float', 'SCHADM': 'float', 'SCHSUP': 'float', 'STUSUP': 'float', 'OTHSUP': 'float', 'RACECAT': 'float'\n",
    "}\n",
    "universe_2010.columns = df_types.keys()\n",
    "\n",
    "# Convert float columns to float\n",
    "for column, dtype in df_types.items():\n",
    "    if dtype == 'float':\n",
    "        universe_2010[column] = universe_2010[column].astype(float)\n",
    "        \n",
    "# universe_2011\n",
    "# Specify types\n",
    "df_types = {\n",
    "    'SURVYEAR': 'str', 'LEAID': 'str', 'FIPST': 'str', 'STID': 'str', 'NAME': 'str', 'PHONE': 'str', 'MSTREE': 'str',\n",
    "    'MCITY': 'str', 'MSTATE': 'str', 'MZIP': 'str', 'MZIP4': 'str', 'LSTREE': 'str', 'LCITY': 'str', 'LSTATE': 'str',\n",
    "    'LZIP': 'str', 'LZIP4': 'str', 'TYPE': 'str', 'UNION': 'str', 'CONUM': 'str', 'CONAME': 'str', 'CSA': 'str',\n",
    "    'CBSA': 'str', 'METMIC': 'str', 'ULOCAL': 'str', 'CDCODE': 'str', 'LATCOD': 'float', 'LONCOD': 'float',\n",
    "    'BIEA': 'str', 'BOUND': 'str', 'GSLO': 'str', 'GSHI': 'str', 'PKOFFRD': 'str', 'KGOFFRD': 'str', 'G01OFFRD': 'str',\n",
    "    'G02OFFRD': 'str', 'G03OFFRD': 'str', 'G04OFFRD': 'str', 'G05OFFRD': 'str', 'G06OFFRD': 'str', 'G07OFFRD': 'str',\n",
    "    'G08OFFRD': 'str', 'G09OFFRD': 'str', 'G10OFFRD': 'str', 'G11OFFRD': 'str', 'G12OFFRD': 'str', 'UGOFFRD': 'str',\n",
    "    'AGCHRT': 'str', 'SCH': 'float', 'SPECED': 'float', 'ELL': 'float', 'PKTCH': 'float', 'KGTCH': 'float',\n",
    "    'ELMTCH': 'float', 'SECTCH': 'float', 'UGTCH': 'float', 'TOTTCH': 'float', 'AIDES': 'float', 'CORSUP': 'float',\n",
    "    'ELMGUI': 'float', 'SECGUI': 'float', 'OTHGUI': 'float', 'TOTGUI': 'float', 'LIBSPE': 'float', 'LIBSUP': 'float',\n",
    "    'LEAADM': 'float', 'LEASUP': 'float', 'SCHADM': 'float', 'SCHSUP': 'float', 'STUSUP': 'float', 'OTHSUP': 'float',\n",
    "    'RACECAT': 'float', 'PK': 'float', 'AMPKM': 'float', 'AMPKF': 'float', 'ASPKM': 'float', 'ASPKF': 'float',\n",
    "    'HIPKM': 'float', 'HIPKF': 'float', 'BLPKM': 'float', 'BLPKF': 'float', 'WHPKM': 'float', 'WHPKF': 'float',\n",
    "    'HPPKM': 'float', 'HPPKF': 'float', 'TRPKM': 'float', 'TRPKF': 'float', 'KG': 'float', 'AMKGM': 'float',\n",
    "    'AMKGF': 'float', 'ASKGM': 'float', 'ASKGF': 'float', 'HIKGM': 'float', 'HIKGF': 'float', 'BLKGM': 'float',\n",
    "    'BLKGF': 'float', 'WHKGM': 'float', 'WHKGF': 'float', 'HPKGM': 'float', 'HPKGF': 'float', 'TRKGM': 'float',\n",
    "    'TRKGF': 'float', 'G01': 'float', 'AM01M': 'float', 'AM01F': 'float', 'AS01M': 'float', 'AS01F': 'float',\n",
    "    'HI01M': 'float', 'HI01F': 'float', 'BL01M': 'float', 'BL01F': 'float', 'WH01M': 'float', 'WH01F': 'float',\n",
    "    'HP01M': 'float', 'HP01F': 'float', 'TR01M': 'float', 'TR01F': 'float', 'G02': 'float', 'AM02M': 'float',\n",
    "    'AM02F': 'float', 'AS02M': 'float', 'AS02F': 'float', 'HI02M': 'float', 'HI02F': 'float', 'BL02M': 'float',\n",
    "    'BL02F': 'float', 'WH02M': 'float', 'WH02F': 'float', 'HP02M': 'float', 'HP02F': 'float', 'TR02M': 'float',\n",
    "    'TR02F': 'float', 'G03': 'float', 'AM03M': 'float', 'AM03F': 'float', 'AS03M': 'float', 'AS03F': 'float',\n",
    "    'HI03M': 'float', 'HI03F': 'float', 'BL03M': 'float', 'BL03F': 'float', 'WH03M': 'float', 'WH03F': 'float',\n",
    "    'HP03M': 'float', 'HP03F': 'float', 'TR03M': 'float', 'TR03F': 'float', 'G04': 'float', 'AM04M': 'float',\n",
    "    'AM04F': 'float', 'AS04M': 'float', 'AS04F': 'float', 'HI04M': 'float', 'HI04F': 'float', 'BL04M': 'float',\n",
    "    'BL04F': 'float', 'WH04M': 'float', 'WH04F': 'float', 'HP04M': 'float', 'HP04F': 'float', 'TR04M': 'float',\n",
    "    'TR04F': 'float', 'G05': 'float', 'AM05M': 'float', 'AM05F': 'float', 'AS05M': 'float', 'AS05F': 'float',\n",
    "    'HI05M': 'float', 'HI05F': 'float', 'BL05M': 'float', 'BL05F': 'float', 'WH05M': 'float', 'WH05F': 'float',\n",
    "    'HP05M': 'float', 'HP05F': 'float', 'TR05M': 'float', 'TR05F': 'float', 'G06': 'float', 'AM06M': 'float',\n",
    "    'AM06F': 'float', 'AS06M': 'float', 'AS06F': 'float', 'HI06M': 'float', 'HI06F': 'float', 'BL06M': 'float',\n",
    "    'BL06F': 'float', 'WH06M': 'float', 'WH06F': 'float', 'HP06M': 'float', 'HP06F': 'float', 'TR06M': 'float',\n",
    "    'TR06F': 'float', 'G07': 'float', 'AM07M': 'float', 'AM07F': 'float', 'AS07M': 'float', 'AS07F': 'float',\n",
    "    'HI07M': 'float', 'HI07F': 'float', 'BL07M': 'float', 'BL07F': 'float', 'WH07M': 'float', 'WH07F': 'float',\n",
    "    'HP07M': 'float', 'HP07F': 'float', 'TR07M': 'float', 'TR07F': 'float', 'G08': 'float', 'AM08M': 'float',\n",
    "    'AM08F': 'float', 'AS08M': 'float', 'AS08F': 'float', 'HI08M': 'float', 'HI08F': 'float', 'BL08M': 'float',\n",
    "    'BL08F': 'float', 'WH08M': 'float', 'WH08F': 'float', 'HP08M': 'float', 'HP08F': 'float', 'TR08M': 'float',\n",
    "    'TR08F': 'float', 'G09': 'float', 'AM09M': 'float', 'AM09F': 'float', 'AS09M': 'float', 'AS09F': 'float',\n",
    "    'HI09M': 'float', 'HI09F': 'float', 'BL09M': 'float', 'BL09F': 'float', 'WH09M': 'float', 'WH09F': 'float',\n",
    "    'HP09M': 'float', 'HP09F': 'float', 'TR09M': 'float', 'TR09F': 'float', 'G100': 'float', 'AM10M': 'float',\n",
    "    'AM10F': 'float', 'AS10M': 'float', 'AS10F': 'float', 'HI10M': 'float', 'HI10F': 'float', 'BL10M': 'float',\n",
    "    'BL10F': 'float', 'WH10M': 'float', 'WH10F': 'float', 'HP10M': 'float', 'HP10F': 'float', 'TR10M': 'float',\n",
    "    'TR10F': 'float', 'G11': 'float', 'AM11M': 'float', 'AM11F': 'float', 'AS11M': 'float', 'AS11F': 'float',\n",
    "    'HI11M': 'float', 'HI11F': 'float', 'BL11M': 'float', 'BL11F': 'float', 'WH11M': 'float', 'WH11F': 'float',\n",
    "    'HP11M': 'float', 'HP11F': 'float', 'TR11M': 'float', 'TR11F': 'float', 'G12': 'float', 'AM12M': 'float',\n",
    "    'AM12F': 'float', 'AS12M': 'float', 'AS12F': 'float', 'HI12M': 'float', 'HI12F': 'float', 'BL12M': 'float',\n",
    "    'BL12F': 'float', 'WH12M': 'float', 'WH12F': 'float', 'HP12M': 'float', 'HP12F': 'float', 'TR12M': 'float',\n",
    "    'TR12F': 'float', 'UG': 'float', 'AMUGM': 'float', 'AMUGF': 'float', 'ASUGM': 'float', 'ASUGF': 'float',\n",
    "    'HIUGM': 'float', 'HIUGF': 'float', 'BLUGM': 'float', 'BLUGF': 'float', 'WHUGM': 'float', 'WHUGF': 'float',\n",
    "    'HPUGM': 'float', 'HPUGF': 'float', 'TRUGM': 'float', 'TRUGF': 'float', 'MEMBER': 'float', 'AM': 'float',\n",
    "    'AMALM': 'float', 'AMALF': 'float', 'ASIAN': 'float', 'ASALM': 'float', 'ASALF': 'float', 'HISP': 'float',\n",
    "    'HIALM': 'float', 'HIALF': 'float', 'BLACK': 'float', 'BLALM': 'float', 'BLALF': 'float', 'WHITE': 'float',\n",
    "    'WHALM': 'float', 'WHALF': 'float', 'PACIFIC': 'float', 'HPALM': 'float', 'HPALF': 'float', 'TR': 'float',\n",
    "    'TRALM': 'float', 'TRALF': 'float', 'TOTETH': 'float'\n",
    "}\n",
    "universe_2011.columns = df_types.keys()\n",
    "\n",
    "# Convert float columns to float\n",
    "for column, dtype in df_types.items():\n",
    "    if dtype == 'float':\n",
    "        universe_2011[column] = universe_2011[column].astype(float)\n",
    "        \n",
    "# finance_2010 and finance_2011\n",
    "# Specify types\n",
    "df_types = {\n",
    "    'LEAID': 'str', 'CENSUSID': 'str', 'FIPST': 'str', 'CONUM': 'str', 'CSA': 'str', 'CBSA': 'str', 'NAME': 'str',\n",
    "    'STNAME': 'str', 'STABBR': 'str', 'SCHLEV': 'str', 'AGCHRT': 'str', 'YEAR': 'str', 'CCDNF': 'str',\n",
    "    'CENFILE': 'str', 'GSLO': 'str', 'GSHI': 'str', 'V33': 'float', 'MEMBERSCH': 'float', 'TOTALREV': 'float',\n",
    "    'TFEDREV': 'float', 'C14': 'float', 'C15': 'float', 'C16': 'float', 'C17': 'float', 'C19': 'float',\n",
    "    'B11': 'float', 'C20': 'float', 'C25': 'float', 'C36': 'float', 'B10': 'float', 'B12': 'float', 'B13': 'float',\n",
    "    'TSTREV': 'float', 'C01': 'float', 'C04': 'float', 'C05': 'float', 'C06': 'float', 'C07': 'float', 'C08': 'float',\n",
    "    'C09': 'float', 'C10': 'float', 'C11': 'float', 'C12': 'float', 'C13': 'float', 'C35': 'float', 'C38': 'float',\n",
    "    'C39': 'float', 'TLOCREV': 'float', 'T02': 'float', 'T06': 'float', 'T09': 'float', 'T15': 'float',\n",
    "    'T40': 'float', 'T99': 'float', 'D11': 'float', 'D23': 'float', 'A07': 'float', 'A08': 'float', 'A09': 'float',\n",
    "    'A11': 'float', 'A13': 'float', 'A15': 'float', 'A20': 'float', 'A40': 'float', 'U11': 'float', 'U22': 'float',\n",
    "    'U30': 'float', 'U50': 'float', 'U97': 'float', 'C24': 'float', 'TOTALEXP': 'float', 'TCURELSC': 'float',\n",
    "    'TCURINST': 'float', 'E13': 'float', 'V91': 'float', 'V92': 'float', 'TCURSSVC': 'float', 'E17': 'float',\n",
    "    'E07': 'float', 'E08': 'float', 'E09': 'float', 'V40': 'float', 'V45': 'float', 'V90': 'float', 'V85': 'float',\n",
    "    'TCUROTH': 'float', 'E11': 'float', 'V60': 'float', 'V65': 'float', 'TNONELSE': 'float', 'V70': 'float',\n",
    "    'V75': 'float', 'V80': 'float', 'TCAPOUT': 'float', 'F12': 'float', 'G15': 'float', 'K09': 'float',\n",
    "    'K10': 'float', 'K11': 'float', 'L12': 'float', 'M12': 'float', 'Q11': 'float', 'I86': 'float', 'Z32': 'float',\n",
    "    'Z33': 'float', 'Z35': 'float', 'Z36': 'float', 'Z37': 'float', 'Z38': 'float', 'V11': 'float', 'V13': 'float',\n",
    "    'V15': 'float', 'V17': 'float', 'V21': 'float', 'V23': 'float', 'V37': 'float', 'V29': 'float', 'Z34': 'float',\n",
    "    'V10': 'float', 'V12': 'float', 'V14': 'float', 'V16': 'float', 'V18': 'float', 'V22': 'float', 'V24': 'float',\n",
    "    'V38': 'float', 'V30': 'float', 'V32': 'float', 'V93': 'float', '_19H': 'float', '_21F': 'float', '_31F': 'float',\n",
    "    '_41F': 'float', '_61V': 'float', '_66V': 'float', 'W01': 'float', 'W31': 'float', 'W61': 'float', 'HR1': 'float',\n",
    "    'HE1': 'float', 'HE2': 'float', 'WEIGHT': 'float', 'FL_V33': 'str', 'FL_MEMBERSC': 'str', 'FL_C14': 'str',\n",
    "    'FL_C15': 'str', 'FL_C16': 'str', 'FL_C17': 'str', 'FL_C19': 'str', 'FL_B11': 'str', 'FL_C20': 'str',\n",
    "    'FL_C25': 'str', 'FL_C36': 'str', 'FL_B10': 'str', 'FL_B12': 'str', 'FL_B13': 'str', 'FL_C01': 'str',\n",
    "    'FL_C04': 'str', 'FL_C05': 'str', 'FL_C06': 'str', 'FL_C07': 'str', 'FL_C08': 'str', 'FL_C09': 'str',\n",
    "    'FL_C10': 'str', 'FL_C11': 'str', 'FL_C12': 'str', 'FL_C13': 'str', 'FL_C35': 'str', 'FL_C38': 'str',\n",
    "    'FL_C39': 'str', 'FL_T02': 'str', 'FL_T06': 'str', 'FL_T09': 'str', 'FL_T15': 'str', 'FL_T40': 'str',\n",
    "    'FL_T99': 'str', 'FL_D11': 'str', 'FL_D23': 'str', 'FL_A07': 'str', 'FL_A08': 'str', 'FL_A09': 'str',\n",
    "    'FL_A11': 'str', 'FL_A13': 'str', 'FL_A15': 'str', 'FL_A20': 'str', 'FL_A40': 'str', 'FL_U11': 'str',\n",
    "    'FL_U22': 'str', 'FL_U30': 'str', 'FL_U50': 'str', 'FL_U97': 'str', 'FL_C24': 'str', 'FL_E13': 'str',\n",
    "    'FL_V91': 'str', 'FL_V92': 'str', 'FL_E17': 'str', 'FL_E07': 'str', 'FL_E08': 'str', 'FL_E09': 'str',\n",
    "    'FL_V40': 'str', 'FL_V45': 'str', 'FL_V90': 'str', 'FL_V85': 'str', 'FL_E11': 'str', 'FL_V60': 'str',\n",
    "    'FL_V65': 'str', 'FL_V70': 'str', 'FL_V75': 'str', 'FL_V80': 'str', 'FL_F12': 'str', 'FL_G15': 'str',\n",
    "    'FL_K09': 'str', 'FL_K10': 'str', 'FL_K11': 'str', 'FL_L12': 'str', 'FL_M12': 'str', 'FL_Q11': 'str',\n",
    "    'FL_I86': 'str', 'FL_Z32': 'str', 'FL_Z33': 'str', 'FL_Z35': 'str', 'FL_Z36': 'str', 'FL_Z37': 'str',\n",
    "    'FL_Z38': 'str', 'FL_V11': 'str', 'FL_V13': 'str', 'FL_V15': 'str', 'FL_V17': 'str', 'FL_V21': 'str',\n",
    "    'FL_V23': 'str', 'FL_V37': 'str', 'FL_V29': 'str', 'FL_Z34': 'str', 'FL_V10': 'str', 'FL_V12': 'str',\n",
    "    'FL_V14': 'str', 'FL_V16': 'str', 'FL_V18': 'str', 'FL_V22': 'str', 'FL_V24': 'str', 'FL_V38': 'str',\n",
    "    'FL_V30': 'str', 'FL_V32': 'str', 'FL_V93': 'str', 'FL_19H': 'str', 'FL_21F': 'str', 'FL_31F': 'str',\n",
    "    'FL_41F': 'str', 'FL_61V': 'str', 'FL_66V': 'str', 'FL_W01': 'str', 'FL_W31': 'str', 'FL_W61': 'str',\n",
    "    'FL_HR1': 'str', 'FL_HE1': 'str', 'FL_HE2': 'str'\n",
    "}\n",
    "finance_2010.columns = df_types.keys()\n",
    "finance_2011.columns = df_types.keys()\n",
    "\n",
    "# Convert float columns to float\n",
    "for column, dtype in df_types.items():\n",
    "    if dtype == 'float':\n",
    "        finance_2011[column] = finance_2011[column].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The financial, universe, and aggregated school universe data will now be combined into a single DataFrame (for each year). I used the \"LEAID\" column (unique identification number for each school district) as a key for the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_data_2010: (18473, 555)\n",
      "merged_data_2011: (18523, 566)\n"
     ]
    }
   ],
   "source": [
    "# Merge 2010 data\n",
    "# Merge finance_2010 and universe_2010\n",
    "merged_data_2010 = pd.merge(\n",
    "    finance_2010,\n",
    "    universe_2010,\n",
    "    how='outer',\n",
    "    on=['LEAID', 'FIPST', 'CONUM', 'CSA', 'CBSA', 'NAME', 'AGCHRT', 'GSLO', 'GSHI']\n",
    ")\n",
    "\n",
    "# Merge school_universe_2010\n",
    "merged_data_2010 = pd.merge(\n",
    "    merged_data_2010,\n",
    "    school_universe_2010.drop(columns=['MEMBER', 'UG']),\n",
    "    how='outer',\n",
    "    on=['LEAID'])\n",
    "\n",
    "# Merge 2011 data\n",
    "merged_data_2011 = pd.merge(\n",
    "    finance_2011,\n",
    "    universe_2011,\n",
    "    how='outer',\n",
    "    on=['LEAID', 'FIPST', 'CONUM', 'CSA', 'CBSA', 'NAME', 'AGCHRT', 'GSLO', 'GSHI']\n",
    ")\n",
    "\n",
    "# Verify DataFrames merged correctly\n",
    "print('merged_data_2010:', merged_data_2010.shape)\n",
    "print('merged_data_2011:', merged_data_2011.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the Data\n",
    "\n",
    "Classification models require [labeled datasets](https://stackoverflow.com/a/19172720). In this case, the labels for each school district represent whether the school district is still operational in five years. To generate labels, I used the universe files from 2015 and 2016. If the school district is listed in the future universe file, and it's status is not \"closed,\" then it is still operational. Otherwise, the school district is no longer operational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 2010 school districts at risk: 0.044841815680880304\n",
      "Proportion of 2011 school districts at risk: 0.04586404586404591\n"
     ]
    }
   ],
   "source": [
    "# Create function to label whether school districts remain operational in five years\n",
    "def calc_five_years_operational(merged_data, directory_data):\n",
    "    \"\"\"\n",
    "    Return pandas dataframe of labeled dataframe\n",
    "\n",
    "    param dataframe merged_data: dataframe of merged data\n",
    "    param dataframe directory_data: dataframe with all school districts in 2015\n",
    "    \"\"\"\n",
    "    \n",
    "    merged_data = merged_data[merged_data['BOUND'] != '2'].copy()\n",
    "    \n",
    "    def condition1_generate(x):\n",
    "        \"\"\"\n",
    "        Return boolean of whether school district exists in 5 years\n",
    "\n",
    "        param string x: LEAID of school district to check\n",
    "        \"\"\"\n",
    "        condition1 = x in directory_data['LEAID'].values\n",
    "        if condition1:\n",
    "            condition1 = directory_data.loc[directory_data['LEAID']==x, 'SY_STATUS'].values != '2'\n",
    "        return condition1\n",
    "\n",
    "    condition1 = merged_data['LEAID'].apply(condition1_generate)\n",
    "    condition2 = merged_data['BOUND'].apply(lambda x: x != '2')\n",
    "    merged_data['exist_five_years'] = condition1 & condition2\n",
    "    return merged_data\n",
    "\n",
    "# Apply function to 2010 and 2011 data\n",
    "labeled_data_2010 = calc_five_years_operational(merged_data_2010, universe_2015_directory)\n",
    "labeled_data_2011 = calc_five_years_operational(merged_data_2011, universe_2016_directory)\n",
    "\n",
    "# Print proportions of school districts that will close within 5 years.\n",
    "print(\n",
    "    'Proportion of 2010 school districts at risk:',\n",
    "    1 - labeled_data_2010['exist_five_years'].sum()/len(labeled_data_2010['exist_five_years'])\n",
    ")\n",
    "print(\n",
    "    'Proportion of 2011 school districts at risk:',\n",
    "    1 - labeled_data_2011['exist_five_years'].sum()/len(labeled_data_2011['exist_five_years'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Data\n",
    "\n",
    "Missing and non-applicable values are coded as negative numbers in the data. These will be converted to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create function to re-encode missing and non-applicable values\n",
    "def encode_missing_values(raw_data):\n",
    "    \"\"\"\n",
    "    Return dataframe with missing and non applicable values re-encoded as NA and 0\n",
    "\n",
    "    param DataFrame raw_data: dataframe without re-encoded missing and non applicable values\n",
    "    \"\"\"\n",
    "    reencoded_data = raw_data.copy()\n",
    "    for column in reencoded_data.columns.tolist():\n",
    "        missing_values = reencoded_data[column].apply(lambda x: x in [-1, '-1', '-1.0', '-1.00', 'M'] or pd.isnull(x))\n",
    "        non_applicable_values = reencoded_data[column].apply(lambda x: x in [-2, '-2', '-2.0', '-2.00', 'N'])\n",
    "        low_quality_values = reencoded_data[column].apply(lambda x: x in [-9, -3, -4, '-9', '-9.0', '-9.00', '-3', '-3.0', '-3.00', '-4', '-4.0', '-4.00'])\n",
    "\n",
    "        reencoded_data.loc[missing_values, column] = np.nan\n",
    "        reencoded_data.loc[non_applicable_values, column] = np.nan\n",
    "        reencoded_data.loc[low_quality_values, column] = np.nan\n",
    "\n",
    "    return reencoded_data\n",
    "\n",
    "# Apply function to 2010 and 2011 data\n",
    "encoded_data_2010 = encode_missing_values(labeled_data_2010)\n",
    "encoded_data_2011 = encode_missing_values(labeled_data_2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Wrangling Tasks\n",
    "\n",
    "Before proceeding to exploratory data analysis, there are a few last steps to complete. These steps will reduce the size of the data we will be analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_data_2010 before: (18175, 556)\n",
      "encoded_data_2011 before: (18315, 567)\n"
     ]
    }
   ],
   "source": [
    "# Print number of columns before wrangling\n",
    "print('encoded_data_2010 before:', encoded_data_2010.shape)\n",
    "print('encoded_data_2011 before:', encoded_data_2011.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both the 2010 and 2011 datasets, there are flag columns representing whether data is missing or non-applicable in other columns. Since I already encoded this information, these columns are no longer needed, and can be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove flag columns\n",
    "encoded_data_2010.drop(\n",
    "    columns=[column for column in encoded_data_2010.columns if column[:3] == 'FL_'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "encoded_data_2011.drop(\n",
    "    columns=[column for column in encoded_data_2011.columns if column[:3] == 'FL_'],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are hundreds of variables attached to each school district, there are some variables which few school districts reported on. I removed variables with more than half the data missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with excessive missing values\n",
    "encoded_data_2010.drop(\n",
    "    columns=[column for column in encoded_data_2010.columns if \n",
    "             pd.isnull(encoded_data_2010[column]).sum()/len(encoded_data_2010) > 0.5],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "encoded_data_2011.drop(\n",
    "    columns=[column for column in encoded_data_2011.columns if \n",
    "             pd.isnull(encoded_data_2011[column]).sum()/len(encoded_data_2011) > 0.5],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the analysis is consistent between the 2010 and 2011 datasets, columns that don't appear in both datasets are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uncommon columns that don't appear in both 2010 and 2011\n",
    "encoded_data_2010.drop(\n",
    "    columns=[column for column in encoded_data_2010.columns if column not in encoded_data_2011.columns],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "encoded_data_2011.drop(\n",
    "    columns=[column for column in encoded_data_2011.columns if column not in encoded_data_2010.columns],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify, I printed the shape of the 2010 and 2011 DataFrames. There should be fewer columns, and the DataFrames should have the same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_data_2010 after: (18175, 356)\n",
      "encoded_data_2011 after: (18315, 356)\n"
     ]
    }
   ],
   "source": [
    "# Print number of columns after wrangling\n",
    "print('encoded_data_2010 after:', encoded_data_2010.shape)\n",
    "print('encoded_data_2011 after:', encoded_data_2011.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Data\n",
    "The processed data are saved to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_2010.to_csv('outputs/encoded_data_2010.csv', index=False)\n",
    "encoded_data_2011.to_csv('outputs/encoded_data_2011.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
